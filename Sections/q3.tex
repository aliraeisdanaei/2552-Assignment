\documentclass[../ass.tex]{subfiles}
\graphicspath{{../Images}}
\addbibresource{../ref.bib}

\begin{document}

\begin{enumerate}
    \item Big \\
        % All of these papers use a very large dataset for their research. 
        The paper on Social Capital \cite{social_cap} uses the largest data of 21 Billion friendships.
        % One of the main points of Salganik was to avoid fixation on size but the importance of the size. 
        With their large dataset, chetty \textit{et al.} were able to have their results at the granularity level of ZIP Code across the continental United States. 
    \item Always On \\
        Hangarten \textit{et al.} \cite{hiring_disc} were able to create a platform for the entire country of Switzerland. 
        This platform functioned as a hiring tool for employers and job-seekers. 
        In this way, they are able to collect the data continuously and study its changes through time.
    \item Non-Reactive \\
        Koenecke's article on racial disparities in automated speech is the least reactive \cite{racial_speech}.
        Due both to the size of its data and its nature, the study is not going to affect the results. 
        The research studied the disparities of accuracies of automated speech tools given different dialects and accents of speech in American English. 
        Because of non-reaction, the researchers could then suggest ways to mitigate the racial disparities found. 
    \item Incomplete \\
        The article of Hangarten \cite{hiring_disc} is the most incomplete. 
        This project measured the time a user's profile was viewed and the results of the contact button being clicked. 
        This is a large operationalisation and assumption. 
        The data does not have explicit information about any discrimination happening, merely other secondary indicators.
    \item Inaccessible \\
        Government arrest records may not always be public; therefore, Kleinburg's paper \cite{machine_pred} to focus on New York arrest records. 
        This simplifies the research and the predictive model.
    \item Non-representative \\
        Chetty's social capital paper \cite{social_cap} is the data of all social capital with regards to Facebook. 
        This is a huge assumption, and the external validity depends on who uses Facebook. 
        However, they justify their data by the sheer size of population considered.
    \item Drifting \\
        Facebook's user base, algorithms, and friendship connections change and drift all the time. 
        Chetty's social capital paper \cite{social_cap} had to deal with drifting of the data over time. 
        The researchers are generally restricted to a snapshot: their results may not hold longitudinally. 
    \item Algorithmic Confounding \\
        Again Chetty's social capital paper \cite{social_cap} is the most algorithmically confounding. 
        This is one of their biggest threats to validity, as the friendships made by the users under question could be motivated by many unknown algorithmic pushes over time. 
    \item Dirty \\
        Once again, Chetty's social capital paper \cite{social_cap} is the most dirty. 
        The other datasets from the other articles do not include as many bots as this dataset. 
        A single valid user may be duplicated many times under pseudonames, spam, fake, and other dirty accounts are very hard to detect. 
        This is a huge challenge and a big threat to validity for this research. 
    \item Sensitive \\
        While the paper on machine predictions \cite{machine_pred} concerns itself with arrests, this information may not necessarily be sensitive.
        As for the most part, this information is often public, even the court hearings allow for public audiences.
        However, the data produced by Chetty in the social paper \cite{social_cap} is at the granularity of ZIP code. 
        This information is coupled with social capital and other sensitive details. 
        Such information could be used to identify individuals, groups, or demographics for malicious intents, such as running scams.
        Such sensitivity is an ethical issue for this research.
\end{enumerate}

\end{document}